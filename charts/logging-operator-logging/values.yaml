# Default values for logging-operator-logging.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Reference name of the logging deployment
loggingRef: ""
# Disable configuration check before deploy
flowConfigCheckDisabled: false
# Use static configuration instead of generated config.
flowConfigOverride:	""

nameOverride: ""
fullnameOverride: ""

# If an immutable field is changed, delete the existing resource
# and recreate it with the new configuration.
enableRecreateWorkloadOnImmutableFieldChange: false

# Fluent-bit configurations https://banzaicloud.com/docs/one-eye/logging-operator/configuration/crds/#fluent-bit-spec
fluentbit: {}

#Node agent Definition
nodeAgents: {}
#  - name: win-agent
#    profile: windows
#    nodeAgentFluentbit:
#      daemonSet:
#        spec:
#          template:
#            spec:
#              containers:
#              - image: banzaicloud/fluentbit:1.8.5
#                name: fluent-bit
#      tls:
#        enabled: false
#  - name: linux-agent
#    profile: linux
#    nodeAgentFluentbit:
#      metrics:
#        prometheusAnnotations: true
#        serviceMonitor: false
#      tls:
#        enabled: false


# Fluentd configurations https://banzaicloud.com/docs/one-eye/logging-operator/configuration/crds/#fluentd-spec
fluentd: {}
# 20Gi persistent storage is configured for fluentd by default.
# Here is an example, on how to override it:
#  bufferStorageVolume:
#    pvc:
#      spec:
#        accessModes:
#          - ReadWriteOnce
#        resources:
#          requests:
#            storage: 40Gi

# Enable secure connection between fluentd and fluent-bit
tls:
  enabled: true
  # Shared key for fluentd authentication
  sharedKey: ""
  fluentbitSecretName: ""
  fluentdSecretName: ""

# Limit namespaces from where to read Flow and Output specs
watchNamespaces: []
# Control namespace that contains ClusterOutput and ClusterFlow resources
controlNamespace: ""

# defaultFlow
defaultFlow: {}

# defaultFlow
globalFilters: []

# ClusterFlows to deploy
clusterFlows: []

# ClusterOutputs to deploy
clusterOutputs: []

# Send all pod logs to kafka
# clusterFlows:
#   - name: all-pods
#     spec:
#       globalOutputRefs:
#         - kafka
# clusterOutputs:
#   - name: kafka
#     spec:
#       kafka:
#         brokers: kafka-headless.kafka.svc.cluster.local:29092
#         format:
#           type: json
#         default_topic: topic
